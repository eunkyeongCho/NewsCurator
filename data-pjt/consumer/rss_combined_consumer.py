from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common.typeinfo import Types
from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer
from pyflink.common.watermark_strategy import WatermarkStrategy
from pyflink.common import Configuration
import json
import os
from elasticsearch import Elasticsearch
import psycopg2
from datetime import datetime
from preprocess import (
    transform_extract_keywords,
    transform_to_embedding,
    transform_classify_category,
    splitfront,
    splitback
)
from dotenv import load_dotenv
from hdfs import InsecureClient
import time

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def create_kafka_source():
    kafka_servers = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")
    kafka_topic = os.getenv("KAFKA_TOPIC", "article")
    kafka_group = os.getenv("KAFKA_GROUP_ID", "flink-combined-group1")
    print(f"Kafka config - servers: {kafka_servers}, topic: {kafka_topic}, group: {kafka_group}")
    return KafkaSource.builder() \
        .set_bootstrap_servers(kafka_servers) \
        .set_topics(kafka_topic) \
        .set_group_id(kafka_group) \
        .set_starting_offsets(KafkaOffsetsInitializer.latest()) \
        .set_value_only_deserializer(SimpleStringSchema()) \
        .build()

def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ìƒì„±í•˜ê³  ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    db_password = os.getenv("DB_PASSWORD")
    if not db_password:
        raise ValueError("DB_PASSWORD environment variable is required")
    pg_conn = psycopg2.connect(
        host=os.getenv("DB_HOST", "localhost"),
        port=int(os.getenv("DB_PORT", 5434)),
        dbname=os.getenv("DB_NAME", "news"),
        user=os.getenv("DB_USERNAME", "ssafyuser"),
        password=db_password
    )
    return pg_conn

def get_es_connection():
    """Elasticsearch ì—°ê²°ì„ ìƒì„±í•˜ê³  ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    es_host = os.getenv("ES_HOST", "localhost")
    es_port = os.getenv("ES_PORT", "9200")
    return Elasticsearch(f"http://{es_host}:{es_port}")

def get_hdfs_client():
    """HDFS í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    hdfs_host = os.getenv("HDFS_HOST", "localhost")
    hdfs_port = os.getenv("HDFS_PORT", "9870")
    hdfs_user = os.getenv("HDFS_USER", "ssafy")
    return InsecureClient(f'http://{hdfs_host}:{hdfs_port}', user=hdfs_user)

def process_and_save_for_hdfs(article, keywords):
    """HDFS ì €ì¥ìš© ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        output_json = json.dumps({
            "write_date": article["write_date"],
            "keywords": keywords
        }, ensure_ascii=False)
        return output_json
    except Exception as e:
        print(f"[âŒ HDFS ì²˜ë¦¬ ì‹¤íŒ¨] {e}")
        return None

def save_to_hdfs(article, keywords, client):
    """ê¸°ì‚¬ë¥¼ HDFSì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    try:
        processed_data = process_and_save_for_hdfs(article, keywords)
        if processed_data:
            # HDFS ë””ë ‰í† ë¦¬ ê²½ë¡œ
            hdfs_dir = '/user/hadoop'
            hdfs_path = f'{hdfs_dir}/articles.json'
            
            # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not client.status(hdfs_dir, strict=False):
                client.makedirs(hdfs_dir)
                print(f"[ğŸ“ HDFS ë””ë ‰í† ë¦¬ ìƒì„±] {hdfs_dir}")
            
            # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            file_exists = client.status(hdfs_path, strict=False) is not None
            
            # ìµœëŒ€ 3ë²ˆê¹Œì§€ retry
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                try:
                    # HDFSì— ë°ì´í„° ì €ì¥ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±, ìˆìœ¼ë©´ append)
                    with client.write(hdfs_path, append=file_exists, overwrite=not file_exists) as writer:
                        writer.write(processed_data + '\n')
                        writer.flush()  # ë²„í¼ ê°•ì œ ë¹„ìš°ê¸°
                    
                    # leaseê°€ í•´ì œë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
                    time.sleep(2)  # 2ì´ˆ ëŒ€ê¸°
                    
                    print(f"[âœ… HDFS ì €ì¥ ì™„ë£Œ] {hdfs_path}")
                    return True
                except Exception as write_error:
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"[âš ï¸ HDFS ì €ì¥ ì¬ì‹œë„ {retry_count}/{max_retries}] {write_error}")
                        time.sleep(3)  # ì‹¤íŒ¨ ì‹œ 3ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    else:
                        raise write_error
            
            return False
    except Exception as e:
        print(f"[âŒ HDFS ì €ì¥ ì‹¤íŒ¨] {e}")
        return False

def process_and_save(article_json):
    """ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•˜ê³  ëª¨ë“  ì €ì¥ì†Œì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # JSON íŒŒì‹±ì€ í•œ ë²ˆë§Œ ìˆ˜í–‰
        article = json.loads(article_json)
        print("\n" + "="*50)
        print(f"ì œëª©: {article['title']}")
        print(f"ì‘ì„±ì: {article['writer']}")
        print(f"ì‘ì„±ì¼: {article['write_date']}")
        print(f"ì¹´í…Œê³ ë¦¬: {article['category']}")
        print(f"URL: {article['url']}")
        print("="*50 + "\n")

        # ì „ì²˜ë¦¬ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
        writer = splitfront(article["writer"]) or 'unknown'
        email = splitback(article["writer"])
        keywords = transform_extract_keywords(article["content"])
        print("[ì„ë² ë”© ë³€í™˜ ì‹œì‘]")
        embedding = transform_to_embedding(article["content"])
        if embedding is None:
            print("[ê²½ê³ ] ì„ë² ë”© ë³€í™˜ ì‹¤íŒ¨")
        else:
            print("[ì„ë² ë”© ë³€í™˜ ì™„ë£Œ]")
        category = transform_classify_category(article["content"])
        current_time = datetime.now()

        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        pg_conn = get_db_connection()
        pg_cursor = pg_conn.cursor()
        es = get_es_connection()
        hdfs_client = get_hdfs_client()

        try:
            # PostgreSQLì— ì €ì¥
            print("[DB ì €ì¥ ì‹œì‘]")
            pg_cursor.execute("""
                INSERT INTO news_article (
                    title, writer, email, write_date, category, content, 
                    url, keywords, embedding, views, updated_at
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (url) DO UPDATE SET
                    title = EXCLUDED.title,
                    writer = EXCLUDED.writer,
                    email = EXCLUDED.email,
                    write_date = EXCLUDED.write_date,
                    category = EXCLUDED.category,
                    content = EXCLUDED.content,
                    keywords = EXCLUDED.keywords,
                    embedding = EXCLUDED.embedding,
                    updated_at = EXCLUDED.updated_at
                RETURNING id
            """, (
                article["title"],
                writer,
                email,
                article["write_date"],
                category,
                article["content"],
                article["url"],
                json.dumps(keywords, ensure_ascii=False),
                embedding,
                0,
                current_time
            ))
            article_id = pg_cursor.fetchone()[0]
            pg_conn.commit()

            # Elasticsearchì— ì €ì¥
            es_doc = {
                "id": article_id,
                "title": article["title"],
                "content": article["content"],
                "writer": writer,
                "category": category,
                "write_date": article["write_date"] + "+00:00",
                "keywords": keywords,
                "url": article["url"],
                "views": 0
            }

            es.update(
                index="news",
                id=str(article_id),
                body={
                    "doc": es_doc,
                    "doc_as_upsert": True
                }
            )

            # HDFSì— ì €ì¥ (ì´ë¯¸ íŒŒì‹±ëœ articleê³¼ keywords ì‚¬ìš©)
            hdfs_success = save_to_hdfs(article, keywords, hdfs_client)

            if hdfs_success:
                print(f"[âœ… ëª¨ë“  ì €ì¥ì†Œ ì €ì¥ ì™„ë£Œ] {article['title']}")
            else:
                print(f"[âš ï¸ ì¼ë¶€ ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨] {article['title']}")

            return article_json

        except Exception as e:
            print(f"[âŒ ì €ì¥ ì‹¤íŒ¨] {e}")
            pg_conn.rollback()
            return None
        finally:
            pg_cursor.close()
            pg_conn.close()

    except Exception as e:
        print(f"[âŒ ì²˜ë¦¬ ì‹¤íŒ¨] {e}")
        return None

def main():
    print("\nInitializing Flink configuration...")
    config = Configuration()
    
    # JAR íŒŒì¼ ê²½ë¡œ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜)
    flink_jar_path = os.getenv("FLINK_JAR_PATH")
    kafka_client_jar_path = os.getenv("KAFKA_CLIENT_JAR_PATH")
    
    print(f"JAR paths:")
    print(f"Flink JAR: {flink_jar_path}")
    print(f"Kafka Client JAR: {kafka_client_jar_path}")
    
    # JAR íŒŒì¼ ê²½ë¡œì— file:// í”„ë¡œí† ì½œ ì¶”ê°€
    if flink_jar_path and not flink_jar_path.startswith('file://'):
        flink_jar_path = f'file://{flink_jar_path}'
    if kafka_client_jar_path and not kafka_client_jar_path.startswith('file://'):
        kafka_client_jar_path = f'file://{kafka_client_jar_path}'
    
    # ì—¬ëŸ¬ JAR íŒŒì¼ì„ ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ ì—°ê²°
    jar_paths = []
    if flink_jar_path:
        jar_paths.append(flink_jar_path)
    if kafka_client_jar_path:
        jar_paths.append(kafka_client_jar_path)
    if jar_paths:
        config.set_string("pipeline.jars", ";".join(jar_paths))
    
    print("\nSetting up Flink environment...")
    env = StreamExecutionEnvironment.get_execution_environment(config)
    
    # ë³‘ë ¬ë„ ì„¤ì • (1ë¡œ ì„¤ì •í•˜ì—¬ ë‹¨ì¼ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰)
    env.set_parallelism(1)
    
    # Kafka ì†ŒìŠ¤ ìƒì„±
    print("Creating Kafka source...")
    kafka_source = create_kafka_source()
    
    print("Setting up data stream...")
    stream = env.from_source(
        source=kafka_source,
        watermark_strategy=WatermarkStrategy.no_watermarks(),
        source_name="Kafka Source"
    )
    
    # ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    processed_stream = stream \
        .map(process_and_save, output_type=Types.STRING()) \
        .filter(lambda x: x is not None)
    
    # Flink ì‘ì—… ì‹¤í–‰
    env.execute("Combined News Article Processing")

if __name__ == "__main__":
    main() 